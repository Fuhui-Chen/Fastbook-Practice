{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15_arch_details.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPk6q++90UEVLRbbPaAWwY4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "249f3ea6dece4b8887e587ada9ad0ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5ee0b65917e248d6a785e06275febacc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d7b873e8eedc4290a375de959519e9c9",
              "IPY_MODEL_e86a2c49ccd849cfa11813b281c3863a"
            ]
          }
        },
        "5ee0b65917e248d6a785e06275febacc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7b873e8eedc4290a375de959519e9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e45557f423f840c08e69d77083f6f1be",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb8dd24a5bc04a8b90b0ebe3acb1728f"
          }
        },
        "e86a2c49ccd849cfa11813b281c3863a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cccadc27c0ed4c8bab38c4a27ddfdeb4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 83.3M/83.3M [00:01&lt;00:00, 59.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_75cb129682e14cf383461d869baa4d80"
          }
        },
        "e45557f423f840c08e69d77083f6f1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb8dd24a5bc04a8b90b0ebe3acb1728f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cccadc27c0ed4c8bab38c4a27ddfdeb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "75cb129682e14cf383461d869baa4d80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fuhui-Chen/Fastbook-Practice/blob/master/15_arch_details.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liG46vmPD8Mx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "outputId": "72684b81-22d0-4942-84b4-a634ec54e6d1"
      },
      "source": [
        "pip install fastai2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastai2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/4f/0f61bb0d376eb47c20430639bac4946ca0cffcd7e693fb86698656324f2d/fastai2-0.0.17-py3-none-any.whl (190kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai2) (2.23.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from fastai2) (2.2.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai2) (3.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fastai2) (0.22.2.post1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai2) (3.13)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from fastai2) (1.5.0+cu101)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fastai2) (7.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai2) (1.4.1)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.6/dist-packages (from fastai2) (0.6.0+cu101)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai2) (1.0.4)\n",
            "Requirement already satisfied: fastprogress>=0.1.22 in /usr/local/lib/python3.6/dist-packages (from fastai2) (0.2.3)\n",
            "Collecting fastcore\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/6e/a18c0ff6cdca36915e65cf1690137134241a33d74ceef7882f4a63a6af55/fastcore-0.1.18-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai2) (2.9)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (47.1.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (0.6.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fastai2) (1.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai2) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fastai2) (0.15.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->fastai2) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai2) (2018.9)\n",
            "Requirement already satisfied: dataclasses>='0.7'; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastcore->fastai2) (0.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fastai2) (1.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->fastai2) (1.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fastai2) (3.1.0)\n",
            "Installing collected packages: fastcore, fastai2\n",
            "Successfully installed fastai2-0.0.17 fastcore-0.1.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1op4plfMEMTq",
        "colab_type": "text"
      },
      "source": [
        "# Application Architectures Deep Dive\n",
        "We are now in the exciting position that we can fully understand the architectures that we have been using for our state-of-the-art models for computer vision, natural language processing, and tabular analysis. In this chapter, we're going to fill in all the missing details on how fastai's application models work and show you how to build the models they use.\n",
        "\n",
        "We will also go back to the custom data preprocessing pipeline we saw in <\n",
        "\n",
        "We'll start with computer vision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOxEuJesEjvr",
        "colab_type": "text"
      },
      "source": [
        "# Computer Vision\n",
        "For computer vision application we use the functions cnn_learner and unet_learner to build our models, depending on the task. In this section we'll explore how to build the Learner objects we used in Parts 1 and 2 of this book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSxu_TQFEoXm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# cnn_learner\n",
        "Let's take a look at what happens when we use the cnn_learner function. We begin by passing this function an architecture to use for the body of the network. Most of the time we use a ResNet, which you already know how to create, so we don't need to delve into that any further. Pretrained weights are downloaded as required and loaded into the ResNet.\n",
        "\n",
        "Then, for transfer learning, the network needs to be cut. This refers to slicing off the final layer, which is only responsible for ImageNet-specific categorization. In fact, we do not slice off only this layer, but everything from the adaptive average pooling layer onwards. The reason for this will become clear in just a moment. Since different architectures might use different types of pooling layers, or even completely different kinds of heads, we don't just search for the adaptive pooling layer to decide where to cut the pretrained model. Instead, we have a dictionary of information that is used for each model to determine where its body ends, and its head starts. We call this model_meta—here it is for resnet-50:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csed1iHYFNO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai2.vision.all import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "551fEUXsEr-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1c419a43-760a-499e-a8b8-dc6a3aaa11e2"
      },
      "source": [
        "model_meta[resnet50]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cut': -2,\n",
              " 'split': <function fastai2.vision.learner._resnet_split>,\n",
              " 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJF-rNGVFbwD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "If we take all of the layers prior to the cut point of -2, we get the part of the model that fastai will keep for transfer learning. Now, we put on our new head. This is created using the function create_head:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3d3Qd7SFceT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "478113c6-1b60-4bde-bf8c-140e169c44d8"
      },
      "source": [
        "\n",
        "#hide_output\n",
        "create_head(20,2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): AdaptiveConcatPool2d(\n",
              "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
              "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
              "  )\n",
              "  (1): Flatten(full=False)\n",
              "  (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (3): Dropout(p=0.25, inplace=False)\n",
              "  (4): Linear(in_features=20, out_features=512, bias=False)\n",
              "  (5): ReLU(inplace=True)\n",
              "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (7): Dropout(p=0.5, inplace=False)\n",
              "  (8): Linear(in_features=512, out_features=2, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3x7WoZoFh-Z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let's now take a look at what unet_learner did in the segmentation problem we showed in <"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ8ZCWhxFpLB",
        "colab_type": "text"
      },
      "source": [
        "# unet_learner\n",
        "One of the most interesting architectures in deep learning is the one that we used for segmentation in <\n",
        "\n",
        "The way we do this is to start with the exact same approach to developing a CNN head as we saw in the previous problem. We start with a ResNet, for instance, and cut off the adaptive pooling layer and everything after that. Then we replace those layers with our custom head, which does the generative task.\n",
        "\n",
        "There was a lot of handwaving in that last sentence! How on earth do we create a CNN head that generates an image? If we start with, say, a 224-pixel input image, then at the end of the ResNet body we will have a 7×7 grid of convolutional activations. How can we convert that into a 224-pixel segmentation mask?\n",
        "\n",
        "Naturally, we do this with a neural network! So we need some kind of layer that can increase the grid size in a CNN. One very simple approach to this is to replace every pixel in the 7×7 grid with four pixels in a 2×2 square. Each of those four pixels will have the same value—this is known as nearest neighbor interpolation. PyTorch provides a layer that does this for us, so one option is to create a head that contains stride-1 convolutional layers (along with batchnorm and ReLU layers as usual) interspersed with 2×2 nearest neighbor interpolation layers. In fact, you can try this now! See if you can create a custom head designed like this, and try it on the CamVid segmentation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g48yrmDFswv",
        "colab_type": "text"
      },
      "source": [
        "# A Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9g7WBy_Fiha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1d0872ee-0257-445d-da0c-1bd487a913bb"
      },
      "source": [
        "path = untar_data(URLs.PETS)\n",
        "files = get_image_files(path/\"images\")\n",
        "\n",
        "class SiameseImage(Tuple):\n",
        "    def show(self, ctx=None, **kwargs): \n",
        "        img1,img2,same_breed = self\n",
        "        if not isinstance(img1, Tensor):\n",
        "            if img2.size != img1.size: img2 = img2.resize(img1.size)\n",
        "            t1,t2 = tensor(img1),tensor(img2)\n",
        "            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)\n",
        "        else: t1,t2 = img1,img2\n",
        "        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n",
        "        return show_image(torch.cat([t1,line,t2], dim=2), \n",
        "                          title=same_breed, ctx=ctx)\n",
        "    \n",
        "def label_func(fname):\n",
        "    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\n",
        "\n",
        "class SiameseTransform(Transform):\n",
        "    def __init__(self, files, label_func, splits):\n",
        "        self.labels = files.map(label_func).unique()\n",
        "        self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels}\n",
        "        self.label_func = label_func\n",
        "        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n",
        "        \n",
        "    def encodes(self, f):\n",
        "        f2,t = self.valid.get(f, self._draw(f))\n",
        "        img1,img2 = PILImage.create(f),PILImage.create(f2)\n",
        "        return SiameseImage(img1, img2, t)\n",
        "    \n",
        "    def _draw(self, f):\n",
        "        same = random.random() < 0.5\n",
        "        cls = self.label_func(f)\n",
        "        if not same: cls = random.choice(L(l for l in self.labels if l != cls)) \n",
        "        return random.choice(self.lbl2files[cls]),same\n",
        "    \n",
        "splits = RandomSplitter()(files)\n",
        "tfm = SiameseTransform(files, label_func, splits)\n",
        "tls = TfmdLists(files, tfm, splits=splits)\n",
        "dls = tls.dataloaders(after_item=[Resize(224), ToTensor], \n",
        "    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D11P280F1A0",
        "colab_type": "text"
      },
      "source": [
        "Using what we just saw, let's build a custom model for this task and train it. How? We will use a pretrained architecture and pass our two images through it. Then we can concatenate the results and send them to a custom head that will return two predictions. In terms of modules, this looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D18nv0FPF3Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class SiameseModel(Module):\n",
        "    def __init__(self, encoder, head):\n",
        "        self.encoder,self.head = encoder,head\n",
        "    \n",
        "    def forward(self, x1, x2):\n",
        "        ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1)\n",
        "        return self.head(ftrs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuUkCJqmF8Sp",
        "colab_type": "text"
      },
      "source": [
        "To create our encoder, we just need to take a pretrained model and cut it, as we explained before. The function create_body does that for us; we just have to pass it the place where we want to cut. As we saw earlier, per the dictionary of metadata for pretrained models, the cut value for a resnet is -2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvABCF_3F-WI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "249f3ea6dece4b8887e587ada9ad0ba8",
            "5ee0b65917e248d6a785e06275febacc",
            "d7b873e8eedc4290a375de959519e9c9",
            "e86a2c49ccd849cfa11813b281c3863a",
            "e45557f423f840c08e69d77083f6f1be",
            "eb8dd24a5bc04a8b90b0ebe3acb1728f",
            "cccadc27c0ed4c8bab38c4a27ddfdeb4",
            "75cb129682e14cf383461d869baa4d80"
          ]
        },
        "outputId": "242d4ba0-cedc-4244-cdfa-8fc660e2e565"
      },
      "source": [
        "\n",
        "encoder = create_body(resnet34, cut=-2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "249f3ea6dece4b8887e587ada9ad0ba8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=87306240.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg0pWb_JGAsC",
        "colab_type": "text"
      },
      "source": [
        "Then we can create our head. A look at the encoder tells us the last layer has 512 features, so this head will need to receive 512*4. Why 4? First we have to multiply by 2 because we have two images. Then we need a second multiplication by 2 because of our concat-pool trick. So we create the head as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auY_emyBGBoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "head = create_head(512*4, 2, ps=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDxw4C9WGEpK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "With our encoder and head, we can now build our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3arS7k9WGHJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SiameseModel(encoder, head)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjngJSe3GIAL",
        "colab_type": "text"
      },
      "source": [
        "Before using Learner, we have two more things to define. First, we must define the loss function we want to use. It's regular cross-entropy, but since our targets are Booleans, we need to convert them to integers or PyTorch will throw an error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVfAO-bnGKMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def loss_func(out, targ):\n",
        "    return nn.CrossEntropyLoss()(out, targ.long())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74eHWeSCGNwi",
        "colab_type": "text"
      },
      "source": [
        "More importantly, to take full advantage of transfer learning, we have to define a custom splitter. A splitter is a function that tells the fastai library how to split the model into parameter groups. These are used behind the scenes to train only the head of a model when we do transfer learning.\n",
        "\n",
        "Here we want two parameter groups: one for the encoder and one for the head. We can thus define the following splitter (params is just a function that returns all parameters of a given module):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np4caqYaGPxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def siamese_splitter(model):\n",
        "    return [params(model.encoder), params(model.head)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br93R8G0GWEK",
        "colab_type": "text"
      },
      "source": [
        "Then we can define our Learner by passing the data, model, loss function, splitter, and any metric we want. Since we are not using a convenience function from fastai for transfer learning (like cnn_learner), we have to call learn.freeze manually. This will make sure only the last parameter group (in this case, the head) is trained:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8_UC5PpGWjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "learn = Learner(dls, model, loss_func=loss_func, \n",
        "                splitter=siamese_splitter, metrics=accuracy)\n",
        "learn.freeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MohDNf2AGYuz",
        "colab_type": "text"
      },
      "source": [
        "Then we can directly train our model with the usual methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqi--GH9GagV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "7f8aaa36-0223-4921-e64f-0139060684f7"
      },
      "source": [
        "learn.fit_one_cycle(4, 3e-3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.540478</td>\n",
              "      <td>0.347153</td>\n",
              "      <td>0.843708</td>\n",
              "      <td>01:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.376155</td>\n",
              "      <td>0.263498</td>\n",
              "      <td>0.896482</td>\n",
              "      <td>01:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.296999</td>\n",
              "      <td>0.201102</td>\n",
              "      <td>0.926252</td>\n",
              "      <td>01:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.249266</td>\n",
              "      <td>0.178211</td>\n",
              "      <td>0.934371</td>\n",
              "      <td>01:21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7akikboGehU",
        "colab_type": "text"
      },
      "source": [
        "Before unfreezing and fine-tuning the whole model a bit more with discriminative learning rates (that is: a lower learning rate for the body and a higher one for the head):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDG7Nq3zGfJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(4, slice(1e-6,1e-4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR5-HQJ0GhX_",
        "colab_type": "text"
      },
      "source": [
        "94.8\\% is very good when we remember a classifier trained the same way (with no data augmentation) had an error rate of 7%.\n",
        "\n",
        "Now that we've seen how to create complete state-of-the-art computer vision models, let's move on to NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecJxm8hwGvZB",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing\n",
        "Converting an AWD-LSTM language model into a transfer learning classifier, as we did in <\n",
        "\n",
        "To create a classifier from this we use an approach described in the ULMFiT paper as \"BPTT for Text Classification (BPT3C)\":\n",
        "\n",
        ": We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences.\n",
        "\n",
        "In other words, the classifier contains a for loop, which loops over each batch of a sequence. The state is maintained across batches, and the activations of each batch are stored. At the end, we use the same average and max concatenated pooling trick that we use for computer vision models—but this time, we do not pool over CNN grid cells, but over RNN sequences.\n",
        "\n",
        "For this for loop we need to gather our data in batches, but each text needs to be treated separately, as they each have their own labels. However, it's very likely that those texts won't all be of the same length, which means we won't be able to put them all in the same array, like we did with the language model.\n",
        "\n",
        "That's where padding is going to help: when grabbing a bunch of texts, we determine the one with the greatest length, then we fill the ones that are shorter with a special token called xxpad. To avoid extreme cases where we have a text with 2,000 tokens in the same batch as a text with 10 tokens (so a lot of padding, and a lot of wasted computation), we alter the randomness by making sure texts of comparable size are put together. The texts will still be in a somewhat random order for the training set (for the validation set we can simply sort them by order of length), but not completely so.\n",
        "\n",
        "This is done automatically behind the scenes by the fastai library when creating our DataLoaders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjG3ZZ1gG0tx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Tabular\n",
        "Finally, let's take a look at fastai.tabular models. (We don't need to look at collaborative filtering separately, since we've already seen that these models are just tabular models, or use the dot product approach, which we've implemented earlier from scratch.)\n",
        "\n",
        "Here is the forward method for TabularModel:\n",
        "\n",
        "if self.n_emb != 0:\n",
        "    x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
        "    x = torch.cat(x, 1)\n",
        "    x = self.emb_drop(x)\n",
        "if self.n_cont != 0:\n",
        "    x_cont = self.bn_cont(x_cont)\n",
        "    x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
        "return self.layers(x)\n",
        "We won't show __init__ here, since it's not that interesting, but will look at each line of code in forwardin turn. The first line:\n",
        "\n",
        "if self.n_emb != 0:\n",
        "is just testing whether there are any embeddings to deal with—we can skip this section if we only have continuous variables. self.embeds contains the embedding matrices, so this gets the activations of each:\n",
        "\n",
        "x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
        "and concatenates them into a single tensor:\n",
        "\n",
        "x = torch.cat(x, 1)\n",
        "Then dropout is applied. You can pass emb_drop to __init__ to change this value:\n",
        "\n",
        "x = self.emb_drop(x)\n",
        "Now we test whether there are any continuous variables to deal with:\n",
        "\n",
        "if self.n_cont != 0:\n",
        "They are passed through a batchnorm layer:\n",
        "\n",
        "x_cont = self.bn_cont(x_cont)\n",
        "and concatenated with the embedding activations, if there were any:\n",
        "\n",
        "x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
        "Finally, this is passed through the linear layers (each of which includes batchnorm, if use_bn is True, and dropout, if ps is set to some value or list of values):\n",
        "\n",
        "return self.layers(x)\n",
        "Congratulations! Now you know every single piece of the architectures used in the fastai library!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CfK30n8G3ii",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Wrapping Up Architectures\n",
        "As you can see, the details of deep learning architectures need not scare you now. You can look inside the code of fastai and PyTorch and see just what is going on. More importantly, try to understand why it's going on. Take a look at the papers that are being referenced in the code, and try to see how the code matches up to the algorithms that are described.\n",
        "\n",
        "Now that we have investigated all of the pieces of a model and the data that is passed into it, we can consider what this means for practical deep learning. If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a really long time. But the reason that deep learning is not straightforward is because your data, memory, and time are typically limited. If you are running out of memory or time, then the solution is to train a smaller model. If you are not able to train for long enough to overfit, then you are not taking advantage of the capacity of your model.\n",
        "\n",
        "So, step one is to get to the point where you can overfit. Then the question is how to reduce that overfitting. <\n",
        "\n",
        "Steps to reducing overfitting\n",
        "\n",
        "Many practitioners, when faced with an overfitting model, start at exactly the wrong end of this diagram. Their starting point is to use a smaller model, or more regularization. Using a smaller model should be absolutely the last step you take, unless training your model is taking up too much time or memory. Reducing the size of your model reduces the ability of your model to learn subtle relationships in your data.\n",
        "\n",
        "Instead, your first step should be to seek to create more data. That could involve adding more labels to data that you already have, finding additional tasks that your model could be asked to solve (or, to think of it another way, identifying different kinds of labels that you could model), or creating additional synthetic data by using more or different data augmentation techniques. Thanks to the development of Mixup and similar approaches, effective data augmentation is now available for nearly all kinds of data.\n",
        "\n",
        "Once you've got as much data as you think you can reasonably get hold of, and are using it as effectively as possible by taking advantage of all the labels that you can find and doing all the augmentation that makes sense, if you are still overfitting you should think about using more generalizable architectures. For instance, adding batch normalization may improve generalization.\n",
        "\n",
        "If you are still overfitting after doing the best you can at using your data and tuning your architecture, then you can take a look at regularization. Generally speaking, adding dropout to the last layer or two will do a good job of regularizing your model. However, as we learned from the story of the development of AWD-LSTM, it is often the case that adding dropout of different types throughout your model can help even more. Generally speaking, a larger model with more regularization is more flexible, and can therefore be more accurate than a smaller model with less regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTybP2NkGlR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}